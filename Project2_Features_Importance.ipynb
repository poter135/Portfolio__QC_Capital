{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5febb32f",
   "metadata": {},
   "source": [
    "### **讀取資料**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcdaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAUUSD 在第2群\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 新的 project_root 指向 common 的上一層\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"QuantCommon\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utils.tools import read_file\n",
    "from utils.processing import get_dollar_bars, apply_cusum_filter, getDailyVol\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clusters = pd.read_csv('clusters.csv', index_col=0)\n",
    "print(f\"XAUUSD 在第{clusters.loc['XAUUSD_M1', 'cluster']}群\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53011ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "group = clusters[clusters['cluster'] == clusters.loc['XAUUSD_M1', 'cluster']]\n",
    "\n",
    "data = dict({})\n",
    "for i in group.index:\n",
    "    filepath = os.path.join(project_root, \"data\", \"FI\", \"M1\",f\"{i}.csv\")\n",
    "    df = pd.read_csv(filepath, index_col=\"time\", parse_dates=True)\n",
    "    df = get_dollar_bars(df)\n",
    "    data[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44fcad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSUM Bars Count: 17589\n"
     ]
    }
   ],
   "source": [
    "vol = getDailyVol(data[\"close\"], span0=20)\n",
    "cusum_events  = apply_cusum_filter(data, volatility=vol).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e9e742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time\n",
       "2021-01-04 00:05:00   2021-01-06 00:07:00\n",
       "2021-01-04 00:19:00   2021-01-06 00:23:00\n",
       "2021-01-04 00:29:00   2021-01-06 00:33:00\n",
       "2021-01-04 00:33:00   2021-01-06 00:33:00\n",
       "2021-01-04 00:56:00   2021-01-06 01:01:00\n",
       "Name: time, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_vertical_barrier(tEvents, close, num_days=2):\n",
    "    t1 = close.index.searchsorted(tEvents + pd.Timedelta(days=num_days))\n",
    "    t1 = t1[t1 < close.shape[0]]\n",
    "    t1 = pd.Series(close.index[t1], index=tEvents[:t1.shape[0]])\n",
    "    return t1\n",
    "vertical_barriers = add_vertical_barrier(cusum_events, data[\"close\"], num_days=2)\n",
    "vertical_barriers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a84647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "\n",
    "def mpPandasObj(func, pdObj, numThreads=24, mpBatches=1, **kargs):\n",
    "    \"\"\"\n",
    "    通用的 multiprocessing 封裝器（支援 Series/DataFrame 分批運算）\n",
    "    pdObj: dict of name→array-like, 會把每個 name 切片後放進 func\n",
    "    \"\"\"\n",
    "    def linParts(numAtoms, numPieces):\n",
    "        edges = np.linspace(0, numAtoms, numPieces + 1)\n",
    "        edges = np.ceil(edges).astype(int)\n",
    "        return list(zip(edges[:-1], edges[1:]))\n",
    "\n",
    "    # 切成 numThreads * mpBatches 段\n",
    "    parts = linParts(len(next(iter(pdObj.values()))), numThreads * mpBatches)\n",
    "    jobs = []\n",
    "    for start, end in parts:\n",
    "        kargs_ = kargs.copy()\n",
    "        for name, arr in pdObj.items():\n",
    "            kargs_[name] = arr[start:end]\n",
    "        jobs.append(delayed(func)(**kargs_))\n",
    "    out = Parallel(n_jobs=numThreads)(jobs)\n",
    "\n",
    "    # 如果回傳的是 DataFrame/Series，就 concat 起來\n",
    "    if isinstance(out[0], (pd.DataFrame, pd.Series)):\n",
    "        return pd.concat(out, axis=0)\n",
    "    return out\n",
    "\n",
    "def applyPtSlOnT1(close, events, pt_sl, molecule):\n",
    "    \"\"\"\n",
    "    在每個事件的 [start, t1] 區間內，找第一次觸及 pt/sl 的時間\n",
    "    回傳包含欄位 ['t1','pt','sl'] 的 DataFrame\n",
    "    \"\"\"\n",
    "    ev = events.loc[molecule]\n",
    "    out = ev[['t1']].copy()\n",
    "    # profit‐taking & stop‐loss 閾值\n",
    "    pt = pd.Series(pt_sl[0] * ev['trgt'], index=ev.index) if pt_sl[0] > 0 else pd.Series(index=ev.index, dtype=float)\n",
    "    sl = pd.Series(-pt_sl[1] * ev['trgt'], index=ev.index) if pt_sl[1] > 0 else pd.Series(index=ev.index, dtype=float)\n",
    "\n",
    "    # 初始化\n",
    "    out['pt'] = pd.NaT\n",
    "    out['sl'] = pd.NaT\n",
    "\n",
    "    for loc, t1 in ev['t1'].fillna(close.index[-1]).items():\n",
    "        path = close.loc[loc:t1]\n",
    "        ret  = (path / close.loc[loc] - 1) * ev.at[loc, 'side']\n",
    "\n",
    "        # 第一次觸及 pt、sl\n",
    "        hits_pt = ret[ret > pt.loc[loc]]\n",
    "        hits_sl = ret[ret < sl.loc[loc]]\n",
    "\n",
    "        out.at[loc, 'pt'] = hits_pt.index[0] if not hits_pt.empty else t1\n",
    "        out.at[loc, 'sl'] = hits_sl.index[0] if not hits_sl.empty else t1\n",
    "\n",
    "    return out\n",
    "\n",
    "def get_events(close,\n",
    "               t_events,\n",
    "               pt_sl,\n",
    "               target,\n",
    "               min_ret=0,\n",
    "               num_threads=None,\n",
    "               vertical_barrier_times=None,\n",
    "               side_prediction=None):\n",
    "    \"\"\"\n",
    "    close: pd.Series 收盤價\n",
    "    t_events: list or index of event start times\n",
    "    pt_sl: [pt_multiplier, sl_multiplier]\n",
    "    target: pd.Series, trgt 大小\n",
    "    min_ret: 最小 trgt 閥值\n",
    "    vertical_barrier_times: pd.Series of same index as t_events (可 None)\n",
    "    side_prediction: pd.Series of same index as t_events (可 None)\n",
    "    \"\"\"\n",
    "    if num_threads is None:\n",
    "        num_threads = mp.cpu_count()\n",
    "\n",
    "    # 1. 篩選 target\n",
    "    target = target.reindex(t_events).dropna()\n",
    "    target = target[target > min_ret]\n",
    "\n",
    "    # 2. 準備垂直障礙\n",
    "    if vertical_barrier_times is None:\n",
    "        vb = pd.Series(close.index[-1], index=t_events)\n",
    "    else:\n",
    "        vb = vertical_barrier_times.reindex(t_events).fillna(close.index[-1])\n",
    "\n",
    "    # 3. 準備 side\n",
    "    if side_prediction is None:\n",
    "        side = pd.Series(1.0, index=target.index)\n",
    "    else:\n",
    "        side = side_prediction.reindex(target.index).fillna(1.0)\n",
    "\n",
    "    # 4. 建 events DataFrame\n",
    "    events = pd.DataFrame({\n",
    "        't1': vb,\n",
    "        'trgt': target,\n",
    "        'side': side\n",
    "    }).dropna(subset=['trgt'])\n",
    "\n",
    "    # 5. 並行計算 pt/sl 觸發時間\n",
    "    df0 = mpPandasObj(func=applyPtSlOnT1,\n",
    "                      pdObj={'molecule': events.index},\n",
    "                      numThreads=num_threads,\n",
    "                      close=close,\n",
    "                      events=events,\n",
    "                      pt_sl=pt_sl)\n",
    "\n",
    "    # 6. 合併並取最早觸發時間\n",
    "    df0 = df0.reindex(events.index)\n",
    "    merged = pd.DataFrame({\n",
    "        't1': events['t1'],\n",
    "        'pt': df0['pt'],\n",
    "        'sl': df0['sl']\n",
    "    })\n",
    "    events['t1'] = merged.min(axis=1)\n",
    "\n",
    "    # 7. 若未提供 side_prediction，就不回傳 side 欄\n",
    "    if side_prediction is None:\n",
    "        events = events.drop(columns=['side'])\n",
    "\n",
    "    return events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae92b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_sl = [1, 1]\n",
    "min_ret = 0.003\n",
    "triple_barrier_events = get_events(close=data['close'],\n",
    "                                               t_events=cusum_events,\n",
    "                                               pt_sl=pt_sl,\n",
    "                                               target=vol,\n",
    "                                               min_ret=min_ret,\n",
    "                                               num_threads=4,\n",
    "                                               vertical_barrier_times=vertical_barriers,\n",
    "                                               side_prediction=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_indicator_matrix_np(events: pd.DataFrame, close_index: pd.DatetimeIndex) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    建立 indicator matrix（純 numpy 版）\n",
    "      - 回傳 shape=(N_events, T_times) 的 0/1 陣列\n",
    "      - events.index 和 events['t1'] 必須都能在 close_index 找到\n",
    "    \"\"\"\n",
    "    # 1. 時間戳到 integer 位置的映射\n",
    "    times = close_index.values  # array of np.datetime64\n",
    "    pos_map = {t: i for i, t in enumerate(times)}\n",
    "    \n",
    "    starts = np.array([pos_map[t] for t in events.index.values], dtype=int)\n",
    "    ends   = np.array([pos_map[t] for t in events['t1'].values], dtype=int)\n",
    "    \n",
    "    N = len(starts)\n",
    "    T = len(times)\n",
    "    ind = np.zeros((N, T), dtype=int)\n",
    "    \n",
    "    # 2. 把每個事件活躍區間 [start, end] 全部設成 1\n",
    "    for i in range(N):\n",
    "        ind[i, starts[i]: ends[i] + 1] = 1\n",
    "    \n",
    "    return ind\n",
    "\n",
    "def average_uniqueness_np(indicator: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    計算每個事件的 average uniqueness\n",
    "      - indicator: shape (N, T)，0/1\n",
    "      - 回傳 shape (N,) 的 weights\n",
    "    \"\"\"\n",
    "    # 每個時間點 t 有幾個事件重疊\n",
    "    overlap = indicator.sum(axis=0)             # (T,)\n",
    "    mask    = overlap > 0                       # 只在 overlap>0 的位置計算\n",
    "    # 瞬時唯一性矩陣 u[i,t]\n",
    "    u = np.zeros_like(indicator, dtype=float)   # (N, T)\n",
    "    u[:, mask] = indicator[:, mask] / overlap[mask]\n",
    "    # 每個事件的持續長度\n",
    "    durations = indicator.sum(axis=1)           # (N,)\n",
    "    # 平均唯一性\n",
    "    weights = u.sum(axis=1) / durations         # (N,)\n",
    "    return weights\n",
    "\n",
    "def sequential_bootstrap_np(indicator: np.ndarray, sample_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sequential Bootstrap（\n",
    "      - indicator: shape (N, T)\n",
    "      - sample_size: 欲抽出的事件數 (通常 = N)\n",
    "      - 回傳 shape (sample_size,) 的 integer indices (0..N-1)\n",
    "    \"\"\"\n",
    "    N = indicator.shape[0]\n",
    "    weights = average_uniqueness_np(indicator)\n",
    "    \n",
    "    phi = np.empty(sample_size, dtype=int)\n",
    "    remaining = np.arange(N)  # 剩下還沒抽的事件\n",
    "    for k in range(sample_size):\n",
    "        # 抽樣 pool\n",
    "        pool = remaining\n",
    "        if k == 0:\n",
    "            probs = np.ones(pool.shape[0], dtype=float)\n",
    "        else:\n",
    "            probs = weights[pool]\n",
    "        probs = probs / probs.sum()\n",
    "        \n",
    "        pick = np.random.choice(pool, p=probs)\n",
    "        phi[k] = pick\n",
    "        # 移除已選事件\n",
    "        remaining = remaining[remaining != pick]\n",
    "    \n",
    "    return phi\n",
    "\n",
    "def cal_weights(triple_barrier_events: pd.DataFrame, close_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    計算每個事件的權重\n",
    "      - triple_barrier_events: DataFrame，包含欄位 ['t1','pt','sl']\n",
    "      - close_series: 收盤價的 Series\n",
    "      - 回傳 DataFrame，包含欄位 ['t1','pt','sl','weight']\n",
    "    \"\"\"\n",
    "    # 1. 建 indicator matrix\n",
    "    ind_mat = get_indicator_matrix_np(triple_barrier_events, close_series.index)\n",
    "    \n",
    "    # 2. 計算每個事件的 weight\n",
    "    weights = average_uniqueness_np(ind_mat)\n",
    "    \n",
    "    # 3. 整理成跟原本一模一樣的 output DataFrame\n",
    "    output = triple_barrier_events.copy()\n",
    "    output['weight'] = weights\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ff0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cal_weights(triple_barrier_events, data['close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fa33aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0    2141\n",
       "-1.0    2028\n",
       "Name: bin, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bins(events, close):\n",
    "    events = events.dropna(subset=['t1'])\n",
    "    px = events.index.union(events['t1'].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "\n",
    "    out = pd.DataFrame(index=events.index)\n",
    "    out['ret'] = px.loc[events['t1'].values].values / px.loc[events.index] - 1\n",
    "\n",
    "    if 'side' in events:\n",
    "        out['ret'] *= events['side']\n",
    "        out['bin'] = -1\n",
    "        out.loc[out['ret'] > 0, 'bin'] = 1\n",
    "    else:\n",
    "        out['bin'] = np.sign(out['ret'])\n",
    "\n",
    "    out = out[out['bin'] != 0]\n",
    "    return out\n",
    "labels  = get_bins(triple_barrier_events, data['close'])\n",
    "labels['bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f6a29",
   "metadata": {},
   "source": [
    "## Make Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c52ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      log_ret     atr_7  bb_width_7  volatility_7  \\\n",
      "time                                                                \n",
      "2024-12-31 15:33:00  0.000394  2.784474    0.005079      0.001177   \n",
      "2024-12-31 15:38:00 -0.000857  2.795264    0.005262      0.001239   \n",
      "2024-12-31 15:44:00 -0.000410  2.717369    0.004973      0.001151   \n",
      "2024-12-31 15:51:00 -0.000031  2.574888    0.004409      0.001147   \n",
      "2024-12-31 15:58:00  0.000341  2.681332    0.001876      0.001052   \n",
      "\n",
      "                           sma_7        ema_7      adx_7  plus_di_7  \\\n",
      "time                                                                  \n",
      "2024-12-31 15:33:00  2610.907143  2612.134702  28.737560  36.590239   \n",
      "2024-12-31 15:38:00  2611.220000  2612.463526  28.121152  31.242004   \n",
      "2024-12-31 15:44:00  2611.787143  2612.442645  26.466139  27.546491   \n",
      "2024-12-31 15:51:00  2612.385714  2612.406984  23.499888  24.917807   \n",
      "2024-12-31 15:58:00  2613.412857  2612.602738  23.076877  27.756102   \n",
      "\n",
      "                     minus_di_7       dx_7  ...  ppo_100  ppo_signal_100  \\\n",
      "time                                        ...                            \n",
      "2024-12-31 15:33:00   17.616890  35.001576  ... -4.72600       -5.066311   \n",
      "2024-12-31 15:38:00   18.977134  24.422701  ... -4.68200       -5.048092   \n",
      "2024-12-31 15:44:00   19.728988  16.536063  ... -4.59990       -5.028629   \n",
      "2024-12-31 15:51:00   22.229299   5.702381  ... -4.48135       -5.007123   \n",
      "2024-12-31 15:58:00   18.297284  20.538811  ... -4.37680       -4.983519   \n",
      "\n",
      "                     ppo_hist_100   kama_100  willr_100  stoch_k_100  \\\n",
      "time                                                                   \n",
      "2024-12-31 15:33:00      0.340311  81.959571  -9.741459    66.903917   \n",
      "2024-12-31 15:38:00      0.366092  81.957820 -20.083102    67.288616   \n",
      "2024-12-31 15:44:00      0.428729  81.962681 -25.023084    67.640611   \n",
      "2024-12-31 15:51:00      0.525773  81.974328 -25.392428    67.571944   \n",
      "2024-12-31 15:58:00      0.606719  81.996136 -21.283472    67.794102   \n",
      "\n",
      "                     stoch_d_100            obv            adl         sar  \n",
      "time                                                                        \n",
      "2024-12-31 15:33:00    56.116585  291942.467890  198868.735447  733.193406  \n",
      "2024-12-31 15:38:00    56.830542  283280.230246  193149.591748  734.103407  \n",
      "2024-12-31 15:44:00    57.534382  276927.206573  187077.529298  734.810160  \n",
      "2024-12-31 15:51:00    58.200596  271338.620069  188167.544717  735.404722  \n",
      "2024-12-31 15:58:00    58.846275  280596.141586  190130.292187  735.921071  \n",
      "\n",
      "[5 rows x 208 columns]\n"
     ]
    }
   ],
   "source": [
    "import talib\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "# --- 以下 FFD helper functions 跟之前一樣 --- #\n",
    "def get_ffd_weights(d: float, size: int, thresh: float = 1e-5) -> np.ndarray:\n",
    "    w = [1.0]\n",
    "    for k in range(1, size):\n",
    "        w.append(w[-1] * ((-d + k - 1) / k))\n",
    "    w = np.array(w)\n",
    "    M = np.where(np.abs(w) > thresh)[0].max() + 1\n",
    "    return w[:M]\n",
    "\n",
    "def fractional_diff(series: pd.Series, d: float, thresh: float = 1e-5) -> pd.Series:\n",
    "    x = series.values\n",
    "    w = get_ffd_weights(d, len(x), thresh)\n",
    "    if w.size == 1:\n",
    "        return series.copy().rename(series.name)\n",
    "    conv = np.convolve(x, w, mode='valid')\n",
    "    idx = series.index[w.size-1:]\n",
    "    return pd.Series(conv, index=idx, name=series.name)\n",
    "\n",
    "def find_min_d(series: pd.Series, d_grid: np.ndarray) -> float:\n",
    "    for d in d_grid:\n",
    "        ffd = fractional_diff(series, d)\n",
    "        pval = adfuller(ffd.dropna(), maxlag=1, regression='c')[1]\n",
    "        if pval < 0.05:\n",
    "            return d\n",
    "    return d_grid[-1]\n",
    "\n",
    "def compute_talib_features(data: pd.DataFrame,\n",
    "                           periods: list = None,\n",
    "                           apply_ffd: bool = True,\n",
    "                           d_vals: np.ndarray = np.linspace(0, 1, 51),\n",
    "                           ffd_thresh: float = 1e-5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    接收含 open, high, low, close, volume 的 DataFrame，\n",
    "    針對 periods 裡每個週期，計算一批 TA-Lib 指標，\n",
    "    並回傳一個新的 DataFrame，裡面是所有這些技術指標特徵。\n",
    "    \"\"\"\n",
    "    if periods is None:\n",
    "        periods = [7, 14, 28, 50, 100]\n",
    "\n",
    "    \n",
    "    high, low, close, volume, log_ret = data['high'], data['low'], data['close'], data['volume'], np.log(data['close']).diff()\n",
    "    # Log Returns\n",
    "    features_df = pd.DataFrame(index=data.index)\n",
    "    features_df['log_ret'] = log_ret\n",
    "    for p in periods:\n",
    "        # —— 波動率類 —— #\n",
    "        features = dict({})\n",
    "        features[f'atr_{p}']      = talib.ATR(high, low, close, timeperiod=p)\n",
    "        upper, mid, lower         = talib.BBANDS(close, timeperiod=p, nbdevup=2, nbdevdn=2)\n",
    "        features[f'bb_width_{p}']  = (upper - lower) / mid\n",
    "        features[f'volatility_{p}'] = log_ret.rolling(window=p, min_periods=p, center=False).std()\n",
    "\n",
    "        # —— 趨勢類 —— #\n",
    "        features[f'sma_{p}']       = talib.SMA(close, timeperiod=p)\n",
    "        features[f'ema_{p}']       = talib.EMA(close, timeperiod=p)\n",
    "        features[f'adx_{p}']       = talib.ADX(high, low, close, timeperiod=p)\n",
    "        features[f'plus_di_{p}']   = talib.PLUS_DI(high, low, close, timeperiod=p)\n",
    "        features[f'minus_di_{p}']  = talib.MINUS_DI(high, low, close, timeperiod=p)\n",
    "        features[f'dx_{p}']        = talib.DX(high, low, close, timeperiod=p)\n",
    "        features[f'adxr_{p}']      = talib.ADXR(high, low, close, timeperiod=p)\n",
    "\n",
    "        # —— 動量／均值回歸類 —— #\n",
    "        features[f'rsi_{p}']       = talib.RSI(close, timeperiod=p)\n",
    "        features[f'roc_{p}']       = talib.ROC(close, timeperiod=p)\n",
    "        features[f'mom_{p}']       = talib.MOM(close, timeperiod=p)\n",
    "        # features[f'autocorr_{p}'] = log_ret.rolling(window=100, min_periods=100, center=False).apply(lambda x: x.autocorr(lag=p), raw=False)\n",
    "        # PPO = (EMA_fast - EMA_slow)/EMA_slow * 100\n",
    "        fast = p\n",
    "        slow = max(2*p, p+1)\n",
    "        features[f'ppo_{p}'], features[f'ppo_signal_{p}'], features[f'ppo_hist_{p}'] = \\\n",
    "            talib.MACDEXT(close,\n",
    "                          fastperiod=fast, fastmatype=0,\n",
    "                          slowperiod=slow, slowmatype=0,\n",
    "                          signalperiod=int(p/2), signalmatype=0)\n",
    "\n",
    "        # KAMA\n",
    "        features[f'kama_{p}']      = talib.KAMA(close, timeperiod=p)\n",
    "\n",
    "        # Williams %R\n",
    "        features[f'willr_{p}']     = talib.WILLR(high, low, close, timeperiod=p)\n",
    "\n",
    "        # Stochastic\n",
    "        slowk, slowd = talib.STOCH(\n",
    "            high, low, close,\n",
    "            fastk_period=p,\n",
    "            slowk_period=max(3, p//3), slowk_matype=0,\n",
    "            slowd_period=max(3, p//3), slowd_matype=0\n",
    "        )\n",
    "        features[f'stoch_k_{p}']   = slowk\n",
    "        features[f'stoch_d_{p}']   = slowd\n",
    "        features_df = pd.concat([features_df, pd.DataFrame(features)], axis=1)\n",
    "\n",
    "    # —— 不需 timeperiod 的指標 —— #\n",
    "    features_df['obv'] = talib.OBV(close, volume)\n",
    "    features_df['adl'] = talib.AD(high, low, close, volume)\n",
    "    features_df['sar'] = talib.SAR(high, low, acceleration=0.02, maximum=0.2)\n",
    "\n",
    "\n",
    "\n",
    "    if apply_ffd:\n",
    "        ffd_dict = {}\n",
    "        for col in features_df.columns:\n",
    "            # 1) 做微分前，自動檢定是否需要平穩化\n",
    "            series = features_df[col].dropna()\n",
    "            # 只對非平穩序列跑 FFD\n",
    "            pval = adfuller(series, maxlag=1, regression='c')[1]\n",
    "            if pval < 0.05:\n",
    "                # 平穩就不動，直接填回原序列\n",
    "                ffd_series = series\n",
    "            else:\n",
    "                # 非平穩就找 d* 並做分數階微分\n",
    "                d_star     = find_min_d(series, d_vals)\n",
    "                ffd_series = fractional_diff(series, d_star, thresh=ffd_thresh)\n",
    "            # ffd_dict[f'{col}_ffd'] = ffd_series\n",
    "            ffd_dict[f'{col}'] = ffd_series\n",
    "\n",
    "        # 合併並對齊 index\n",
    "        ffd_df = pd.DataFrame(ffd_dict)\n",
    "        features_df = pd.concat([features_df, ffd_df], axis=1)\n",
    "        features_df = features_df.dropna()\n",
    "    return features_df\n",
    "\n",
    "# 使用示範\n",
    "feats = compute_talib_features(data,\n",
    "                               periods=[7,14,28,50,100],\n",
    "                               apply_ffd=True)\n",
    "# print(feats.filter(like='_ffd').tail())\n",
    "print(feats.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462f8ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4122, 208) (4122,) (4122,) (4122,)\n"
     ]
    }
   ],
   "source": [
    "idx = feats.index.intersection(labels.index)\n",
    "feats = feats.loc[idx]\n",
    "labels = labels.loc[idx]['bin']\n",
    "weights =  weights.loc[idx]['weight']\n",
    "t1 = triple_barrier_events.loc[idx]['t1']\n",
    "print(feats.shape, labels.shape, weights.shape, t1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b3aad",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83b73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "feats = feats.dropna()\n",
    "class RollingPercentileTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    對每一欄做滑動 percentile 計算，回傳每個時間點 t \n",
    "    欄位值在過去 window 期內的百分位 (0~1)。\n",
    "    \"\"\"\n",
    "    def __init__(self, window: int = 252, min_periods: int = 1):\n",
    "        self.window = window\n",
    "        self.min_periods = min_periods\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 不需要學任何東西\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # 假設 X 是 DataFrame\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for col in X.columns:\n",
    "            # 每個 col 分別做 rolling.apply\n",
    "            X[col] = (\n",
    "                X[col]\n",
    "                .rolling(window=self.window, min_periods=self.min_periods)\n",
    "                .apply(lambda arr: (arr <= arr[-1]).sum() / len(arr), raw=True)\n",
    "            )\n",
    "        return X.values  # 回傳 numpy array 給後續 scaler\n",
    "\n",
    "# === Pipeline 1: rolling percentile → z-score → PCA ===\n",
    "pipe1 = Pipeline([\n",
    "    ('roll_pct', RollingPercentileTransformer(window=252)),\n",
    "    ('scaler',  StandardScaler()),\n",
    "    ('pca',     PCA(n_components=0.95, whiten=False)),\n",
    "])\n",
    "\n",
    "# === Pipeline 2: z-score → PCA ===\n",
    "pipe2 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca',    PCA(n_components=0.95, whiten=False)),\n",
    "])\n",
    "\n",
    "# === 使用方式 ===\n",
    "# 假設 feats 是一個 DataFrame，columns 就是你的所有技術指標\n",
    "# e.g. feats = compute_talib_features(data)\n",
    "\n",
    "# 1) 第一條流水線\n",
    "X1 = pipe1.fit_transform(feats)  \n",
    "# 2) 第二條流水線\n",
    "X2 = pipe2.fit_transform(feats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4fde4",
   "metadata": {},
   "source": [
    "## MDA MDI SFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226a295",
   "metadata": {},
   "source": [
    "#### Purged K Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d762fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PurgedKFold:\n",
    "    def __init__(self, n_splits=3, t1=None, pct_embargo=0.0):\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError(\"t1 must be a pandas Series\")\n",
    "        self.n_splits = n_splits\n",
    "        self.t1 = t1.sort_index()\n",
    "        self.pct_embargo = pct_embargo\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if not X.index.equals(self.t1.index):\n",
    "            raise ValueError(\"X and t1 must have the same index\")\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        # divide indices into contiguous chunks\n",
    "        test_slices = np.array_split(indices, self.n_splits)\n",
    "        mbrg = int(n_samples * self.pct_embargo)\n",
    "\n",
    "        for slice_ in test_slices:\n",
    "            i, j = slice_[0], slice_[-1] + 1\n",
    "            test_idx = indices[i:j]\n",
    "\n",
    "            # start‐time of test block\n",
    "            t0 = self.t1.index[i]\n",
    "            # end‐time of test block\n",
    "            t1_max = self.t1.iloc[test_idx].max()\n",
    "            # find the position just after t1_max\n",
    "            max_t1_pos = self.t1.index.searchsorted(t1_max)\n",
    "\n",
    "            # training before test block\n",
    "            train_before = indices[self.t1.index < t0]\n",
    "            # training after test + embargo\n",
    "            train_after = indices[max_t1_pos + mbrg :]\n",
    "\n",
    "            train_idx = np.concatenate([train_before, train_after])\n",
    "            yield train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4193bf1",
   "metadata": {},
   "source": [
    "#### CVscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "def cv_score(clf,\n",
    "             X,\n",
    "             y,\n",
    "             sample_weight=None,\n",
    "             scoring='neg_log_loss',\n",
    "             t1=None,\n",
    "             cv=3,\n",
    "             pct_embargo=0.01):\n",
    "\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise ValueError(\"scoring must be 'neg_log_loss' or 'accuracy'\")\n",
    "\n",
    "    pkf = PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in pkf.split(X):\n",
    "        # 複製一份新的 model\n",
    "        model = clone(clf)\n",
    "        # fit\n",
    "        if sample_weight is None:\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        else:\n",
    "            model.fit(X.iloc[train_idx],\n",
    "                      y.iloc[train_idx],\n",
    "                      sample_weight=sample_weight.iloc[train_idx].values)\n",
    "        # predict + score\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = model.predict_proba(X.iloc[test_idx])\n",
    "            sc = -log_loss(y.iloc[test_idx],\n",
    "                           prob,\n",
    "                           sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values),\n",
    "                           labels=model.classes_)\n",
    "        else:\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            sc = accuracy_score(y.iloc[test_idx],\n",
    "                                pred,\n",
    "                                sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values))\n",
    "        scores.append(sc)\n",
    "    return np.array(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972226d",
   "metadata": {},
   "source": [
    "#### MDA MDI SFI 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58a2b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假設你已經有：\n",
    "#   - PurgedKFold 實作  \n",
    "#   - cv_score 函式  \n",
    "# 並且都在 your_module 裡可以 import  \n",
    "\n",
    "\n",
    "# 1) MDI Feature Importance\n",
    "def feat_imp_mdi(fit, feat_names):\n",
    "    \"\"\"\n",
    "    fit: 已訓練好的 tree‐ensemble（RandomForest, ExtraTrees…）\n",
    "    feat_names: list of feature names\n",
    "    return: pd.DataFrame with columns ['mean','std'] 純量化後的重要度\n",
    "    \"\"\"\n",
    "    # 從每顆樹蒐集 feature_importances_\n",
    "    df0 = pd.DataFrame(\n",
    "        [tree.feature_importances_ for tree in fit.estimators_],\n",
    "        columns=feat_names\n",
    "    ).replace(0, np.nan)  # 如果 max_features=1，某些 tree 有 0\n",
    "    imp = pd.concat({\n",
    "        'median': df0.median(),\n",
    "        'std' : df0.std() * df0.shape[0]**-0.5\n",
    "    }, axis=1)\n",
    "    # normalize to sum=1\n",
    "    imp['median'] /= imp['median'].sum()\n",
    "    imp.sort_values(by='median', ascending=False, inplace=True)\n",
    "    return imp\n",
    "\n",
    "\n",
    "# 2) MDA: 支援 X, y, sample_weight, t1 為 np.ndarray\n",
    "def feat_imp_mda(clf,\n",
    "                 X,\n",
    "                 y,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = 'neg_log_loss'\n",
    "                ) -> (pd.DataFrame, float):\n",
    "    # --- 1) numpy → pandas ---\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    feat_names = list(X.columns)\n",
    "\n",
    "    # --- 2) baseline score ---\n",
    "    base_scores = cv_score(clf, X, y,\n",
    "                           sample_weight=sample_weight,\n",
    "                           scoring=scoring,\n",
    "                           t1=t1,\n",
    "                           cv=cv,\n",
    "                           pct_embargo=pct_embargo)\n",
    "    base_mean = base_scores.mean()\n",
    "\n",
    "    # --- 3) 每個 feature permutation, 加進度條 ---\n",
    "    diffs = []\n",
    "    for col in tqdm(feat_names, desc=\"MDA permuting features\"):\n",
    "        Xp = X.copy()\n",
    "        np.random.shuffle(Xp[col].values)\n",
    "        perm_scores = cv_score(clf, Xp, y,\n",
    "                               sample_weight=sample_weight,\n",
    "                               scoring=scoring,\n",
    "                               t1=t1,\n",
    "                               cv=cv,\n",
    "                               pct_embargo=pct_embargo)\n",
    "        diffs.append(base_scores - perm_scores)\n",
    "\n",
    "    diffs = np.vstack(diffs)\n",
    "    imp_df = pd.DataFrame({\n",
    "        'mean': diffs.mean(axis=1),\n",
    "        'std' : diffs.std(axis=1) * diffs.shape[1]**-0.5\n",
    "    }, index=feat_names)\n",
    "    imp_df.sort_values(by='mean', ascending=False, inplace=True)\n",
    "    return imp_df, base_mean\n",
    "\n",
    "# 3) SFI: 支援 X, y, sample_weight, t1 為 np.ndarray\n",
    "def SFI(feat_names: list,\n",
    "                 clf,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = 'neg_log_loss'\n",
    "                ) -> pd.DataFrame:\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X, columns=feat_names)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    imp = pd.DataFrame(columns=['mean', 'std'])\n",
    "    for featName in feat_names:\n",
    "        dfo = cv_score(clf, X=X[[featName]],  y = y,\n",
    "                      sample_weight= sample_weight,\n",
    "                      scoring=scoring, t1 = t1, cv = cv)\n",
    "        imp.loc[featName, 'mean'] = dfo.mean()\n",
    "        imp.loc[featName, 'std'] = dfo.std() * dfo.shape[0]**-0.5\n",
    "        imp.sort_values(by='mean', ascending=False, inplace=True)\n",
    "    return imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630b0e3",
   "metadata": {},
   "source": [
    "### RF and compute MDI MDA SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c723464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [f\"PCA_{i}\" for i in range(X2.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "963a12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用CV不用切割資料集\n",
    "X = pd.DataFrame(X2, columns= col, index=feats.index)\n",
    "y = labels.values\n",
    "weights = weights.values\n",
    "# t1 在上面定義好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1a5bfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCA_0</th>\n",
       "      <th>PCA_1</th>\n",
       "      <th>PCA_2</th>\n",
       "      <th>PCA_3</th>\n",
       "      <th>PCA_4</th>\n",
       "      <th>PCA_5</th>\n",
       "      <th>PCA_6</th>\n",
       "      <th>PCA_7</th>\n",
       "      <th>PCA_8</th>\n",
       "      <th>PCA_9</th>\n",
       "      <th>PCA_10</th>\n",
       "      <th>PCA_11</th>\n",
       "      <th>PCA_12</th>\n",
       "      <th>PCA_13</th>\n",
       "      <th>PCA_14</th>\n",
       "      <th>PCA_15</th>\n",
       "      <th>PCA_16</th>\n",
       "      <th>PCA_17</th>\n",
       "      <th>PCA_18</th>\n",
       "      <th>PCA_19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-22 16:00:00</th>\n",
       "      <td>-5.799092</td>\n",
       "      <td>-6.667734</td>\n",
       "      <td>2.831685</td>\n",
       "      <td>-4.376821</td>\n",
       "      <td>4.745149</td>\n",
       "      <td>4.974754</td>\n",
       "      <td>2.585366</td>\n",
       "      <td>-0.389366</td>\n",
       "      <td>4.162528</td>\n",
       "      <td>1.293853</td>\n",
       "      <td>1.544921</td>\n",
       "      <td>-1.215628</td>\n",
       "      <td>-2.283115</td>\n",
       "      <td>1.045526</td>\n",
       "      <td>-3.037757</td>\n",
       "      <td>1.378900</td>\n",
       "      <td>1.650350</td>\n",
       "      <td>-2.139362</td>\n",
       "      <td>-0.283402</td>\n",
       "      <td>-0.148872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28 14:50:00</th>\n",
       "      <td>-12.438673</td>\n",
       "      <td>-5.509063</td>\n",
       "      <td>0.436969</td>\n",
       "      <td>-2.253595</td>\n",
       "      <td>4.779983</td>\n",
       "      <td>-4.238147</td>\n",
       "      <td>-5.008632</td>\n",
       "      <td>-0.920011</td>\n",
       "      <td>4.083533</td>\n",
       "      <td>0.551789</td>\n",
       "      <td>0.355830</td>\n",
       "      <td>0.725524</td>\n",
       "      <td>1.191631</td>\n",
       "      <td>0.931340</td>\n",
       "      <td>1.555752</td>\n",
       "      <td>0.790570</td>\n",
       "      <td>-0.931003</td>\n",
       "      <td>-1.237774</td>\n",
       "      <td>-2.571104</td>\n",
       "      <td>-0.070142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28 15:58:00</th>\n",
       "      <td>-0.914997</td>\n",
       "      <td>-3.263566</td>\n",
       "      <td>-6.273350</td>\n",
       "      <td>5.583443</td>\n",
       "      <td>2.726280</td>\n",
       "      <td>2.424141</td>\n",
       "      <td>2.093870</td>\n",
       "      <td>1.179466</td>\n",
       "      <td>-0.341848</td>\n",
       "      <td>-0.887487</td>\n",
       "      <td>3.766684</td>\n",
       "      <td>0.869564</td>\n",
       "      <td>1.103646</td>\n",
       "      <td>1.653496</td>\n",
       "      <td>-1.123067</td>\n",
       "      <td>0.720345</td>\n",
       "      <td>0.562026</td>\n",
       "      <td>-1.178783</td>\n",
       "      <td>0.067104</td>\n",
       "      <td>-2.912669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28 16:15:00</th>\n",
       "      <td>-4.921787</td>\n",
       "      <td>-4.013460</td>\n",
       "      <td>-4.221147</td>\n",
       "      <td>2.292808</td>\n",
       "      <td>3.155357</td>\n",
       "      <td>-1.735619</td>\n",
       "      <td>3.362364</td>\n",
       "      <td>-3.007337</td>\n",
       "      <td>-0.953520</td>\n",
       "      <td>1.139383</td>\n",
       "      <td>3.295104</td>\n",
       "      <td>-0.222116</td>\n",
       "      <td>1.022648</td>\n",
       "      <td>-0.279718</td>\n",
       "      <td>-0.456692</td>\n",
       "      <td>0.641452</td>\n",
       "      <td>-0.455475</td>\n",
       "      <td>-1.646982</td>\n",
       "      <td>-0.944321</td>\n",
       "      <td>-0.659289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28 16:21:00</th>\n",
       "      <td>-8.490138</td>\n",
       "      <td>-4.423071</td>\n",
       "      <td>-2.284867</td>\n",
       "      <td>0.825572</td>\n",
       "      <td>4.501388</td>\n",
       "      <td>-2.716232</td>\n",
       "      <td>2.577636</td>\n",
       "      <td>-2.354038</td>\n",
       "      <td>0.865607</td>\n",
       "      <td>0.408851</td>\n",
       "      <td>2.383079</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>1.365664</td>\n",
       "      <td>1.509007</td>\n",
       "      <td>-1.289653</td>\n",
       "      <td>0.346439</td>\n",
       "      <td>-0.605571</td>\n",
       "      <td>-1.815647</td>\n",
       "      <td>-1.195692</td>\n",
       "      <td>-0.297338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-20 16:32:00</th>\n",
       "      <td>-10.078947</td>\n",
       "      <td>10.976820</td>\n",
       "      <td>-2.200420</td>\n",
       "      <td>-5.326469</td>\n",
       "      <td>3.837847</td>\n",
       "      <td>0.166117</td>\n",
       "      <td>-1.960596</td>\n",
       "      <td>-0.426028</td>\n",
       "      <td>2.010870</td>\n",
       "      <td>-1.870346</td>\n",
       "      <td>0.778321</td>\n",
       "      <td>-1.961269</td>\n",
       "      <td>1.455997</td>\n",
       "      <td>1.611769</td>\n",
       "      <td>-0.650180</td>\n",
       "      <td>-2.265725</td>\n",
       "      <td>-0.386517</td>\n",
       "      <td>3.155148</td>\n",
       "      <td>0.285252</td>\n",
       "      <td>0.630826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-20 17:18:00</th>\n",
       "      <td>-11.826614</td>\n",
       "      <td>11.489787</td>\n",
       "      <td>-0.894275</td>\n",
       "      <td>-3.255975</td>\n",
       "      <td>6.131481</td>\n",
       "      <td>1.406690</td>\n",
       "      <td>-2.766024</td>\n",
       "      <td>-0.444787</td>\n",
       "      <td>1.300437</td>\n",
       "      <td>-1.161436</td>\n",
       "      <td>-0.928581</td>\n",
       "      <td>-2.642621</td>\n",
       "      <td>1.669947</td>\n",
       "      <td>1.000449</td>\n",
       "      <td>-0.558620</td>\n",
       "      <td>-2.406410</td>\n",
       "      <td>-0.332787</td>\n",
       "      <td>3.178175</td>\n",
       "      <td>0.084099</td>\n",
       "      <td>0.079467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-23 07:34:00</th>\n",
       "      <td>-6.772874</td>\n",
       "      <td>11.546762</td>\n",
       "      <td>-5.528706</td>\n",
       "      <td>-6.310889</td>\n",
       "      <td>3.496664</td>\n",
       "      <td>-0.622073</td>\n",
       "      <td>2.408875</td>\n",
       "      <td>1.316003</td>\n",
       "      <td>1.649202</td>\n",
       "      <td>0.434992</td>\n",
       "      <td>0.721245</td>\n",
       "      <td>-2.058231</td>\n",
       "      <td>2.431842</td>\n",
       "      <td>0.244633</td>\n",
       "      <td>-1.828676</td>\n",
       "      <td>-1.167709</td>\n",
       "      <td>0.122303</td>\n",
       "      <td>2.939340</td>\n",
       "      <td>0.339328</td>\n",
       "      <td>1.005489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 03:22:00</th>\n",
       "      <td>-5.937019</td>\n",
       "      <td>10.048670</td>\n",
       "      <td>-1.800176</td>\n",
       "      <td>-10.201551</td>\n",
       "      <td>3.500005</td>\n",
       "      <td>0.787053</td>\n",
       "      <td>-2.101928</td>\n",
       "      <td>-0.470903</td>\n",
       "      <td>1.261134</td>\n",
       "      <td>1.725928</td>\n",
       "      <td>2.235624</td>\n",
       "      <td>0.501275</td>\n",
       "      <td>0.568966</td>\n",
       "      <td>1.236081</td>\n",
       "      <td>-0.501452</td>\n",
       "      <td>0.882439</td>\n",
       "      <td>0.391568</td>\n",
       "      <td>2.427200</td>\n",
       "      <td>0.284950</td>\n",
       "      <td>0.320626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-30 07:32:00</th>\n",
       "      <td>-1.011441</td>\n",
       "      <td>10.950752</td>\n",
       "      <td>-7.491749</td>\n",
       "      <td>-6.394639</td>\n",
       "      <td>1.140040</td>\n",
       "      <td>5.408082</td>\n",
       "      <td>-1.633926</td>\n",
       "      <td>0.626959</td>\n",
       "      <td>-1.054929</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>3.083748</td>\n",
       "      <td>-0.097350</td>\n",
       "      <td>-0.329954</td>\n",
       "      <td>-0.219577</td>\n",
       "      <td>0.462693</td>\n",
       "      <td>1.203752</td>\n",
       "      <td>0.288132</td>\n",
       "      <td>2.913223</td>\n",
       "      <td>-0.352685</td>\n",
       "      <td>0.409964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4122 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         PCA_0      PCA_1     PCA_2      PCA_3     PCA_4  \\\n",
       "time                                                                       \n",
       "2021-01-22 16:00:00  -5.799092  -6.667734  2.831685  -4.376821  4.745149   \n",
       "2021-01-28 14:50:00 -12.438673  -5.509063  0.436969  -2.253595  4.779983   \n",
       "2021-01-28 15:58:00  -0.914997  -3.263566 -6.273350   5.583443  2.726280   \n",
       "2021-01-28 16:15:00  -4.921787  -4.013460 -4.221147   2.292808  3.155357   \n",
       "2021-01-28 16:21:00  -8.490138  -4.423071 -2.284867   0.825572  4.501388   \n",
       "...                        ...        ...       ...        ...       ...   \n",
       "2024-12-20 16:32:00 -10.078947  10.976820 -2.200420  -5.326469  3.837847   \n",
       "2024-12-20 17:18:00 -11.826614  11.489787 -0.894275  -3.255975  6.131481   \n",
       "2024-12-23 07:34:00  -6.772874  11.546762 -5.528706  -6.310889  3.496664   \n",
       "2024-12-30 03:22:00  -5.937019  10.048670 -1.800176 -10.201551  3.500005   \n",
       "2024-12-30 07:32:00  -1.011441  10.950752 -7.491749  -6.394639  1.140040   \n",
       "\n",
       "                        PCA_5     PCA_6     PCA_7     PCA_8     PCA_9  \\\n",
       "time                                                                    \n",
       "2021-01-22 16:00:00  4.974754  2.585366 -0.389366  4.162528  1.293853   \n",
       "2021-01-28 14:50:00 -4.238147 -5.008632 -0.920011  4.083533  0.551789   \n",
       "2021-01-28 15:58:00  2.424141  2.093870  1.179466 -0.341848 -0.887487   \n",
       "2021-01-28 16:15:00 -1.735619  3.362364 -3.007337 -0.953520  1.139383   \n",
       "2021-01-28 16:21:00 -2.716232  2.577636 -2.354038  0.865607  0.408851   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2024-12-20 16:32:00  0.166117 -1.960596 -0.426028  2.010870 -1.870346   \n",
       "2024-12-20 17:18:00  1.406690 -2.766024 -0.444787  1.300437 -1.161436   \n",
       "2024-12-23 07:34:00 -0.622073  2.408875  1.316003  1.649202  0.434992   \n",
       "2024-12-30 03:22:00  0.787053 -2.101928 -0.470903  1.261134  1.725928   \n",
       "2024-12-30 07:32:00  5.408082 -1.633926  0.626959 -1.054929  0.907542   \n",
       "\n",
       "                       PCA_10    PCA_11    PCA_12    PCA_13    PCA_14  \\\n",
       "time                                                                    \n",
       "2021-01-22 16:00:00  1.544921 -1.215628 -2.283115  1.045526 -3.037757   \n",
       "2021-01-28 14:50:00  0.355830  0.725524  1.191631  0.931340  1.555752   \n",
       "2021-01-28 15:58:00  3.766684  0.869564  1.103646  1.653496 -1.123067   \n",
       "2021-01-28 16:15:00  3.295104 -0.222116  1.022648 -0.279718 -0.456692   \n",
       "2021-01-28 16:21:00  2.383079  0.962312  1.365664  1.509007 -1.289653   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2024-12-20 16:32:00  0.778321 -1.961269  1.455997  1.611769 -0.650180   \n",
       "2024-12-20 17:18:00 -0.928581 -2.642621  1.669947  1.000449 -0.558620   \n",
       "2024-12-23 07:34:00  0.721245 -2.058231  2.431842  0.244633 -1.828676   \n",
       "2024-12-30 03:22:00  2.235624  0.501275  0.568966  1.236081 -0.501452   \n",
       "2024-12-30 07:32:00  3.083748 -0.097350 -0.329954 -0.219577  0.462693   \n",
       "\n",
       "                       PCA_15    PCA_16    PCA_17    PCA_18    PCA_19  \n",
       "time                                                                   \n",
       "2021-01-22 16:00:00  1.378900  1.650350 -2.139362 -0.283402 -0.148872  \n",
       "2021-01-28 14:50:00  0.790570 -0.931003 -1.237774 -2.571104 -0.070142  \n",
       "2021-01-28 15:58:00  0.720345  0.562026 -1.178783  0.067104 -2.912669  \n",
       "2021-01-28 16:15:00  0.641452 -0.455475 -1.646982 -0.944321 -0.659289  \n",
       "2021-01-28 16:21:00  0.346439 -0.605571 -1.815647 -1.195692 -0.297338  \n",
       "...                       ...       ...       ...       ...       ...  \n",
       "2024-12-20 16:32:00 -2.265725 -0.386517  3.155148  0.285252  0.630826  \n",
       "2024-12-20 17:18:00 -2.406410 -0.332787  3.178175  0.084099  0.079467  \n",
       "2024-12-23 07:34:00 -1.167709  0.122303  2.939340  0.339328  1.005489  \n",
       "2024-12-30 03:22:00  0.882439  0.391568  2.427200  0.284950  0.320626  \n",
       "2024-12-30 07:32:00  1.203752  0.288132  2.913223 -0.352685  0.409964  \n",
       "\n",
       "[4122 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1af35c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf0c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgU = weights.mean()\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_features='auto', class_weight='balanced')\n",
    "clf = BaggingClassifier(estimator=clf, n_estimators=1000, max_samples=avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c73d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          median       std\n",
      "PCA_10  0.053839  0.000421\n",
      "PCA_15  0.053119  0.000404\n",
      "PCA_4   0.051702  0.000404\n",
      "PCA_17  0.051656  0.000406\n",
      "PCA_1   0.051380  0.000398\n",
      "PCA_6   0.051320  0.000412\n",
      "PCA_14  0.050161  0.000403\n",
      "PCA_19  0.050110  0.000400\n",
      "PCA_3   0.050083  0.000413\n",
      "PCA_16  0.049945  0.000398\n",
      "PCA_18  0.049815  0.000406\n",
      "PCA_11  0.049576  0.000404\n",
      "PCA_13  0.049454  0.000399\n",
      "PCA_5   0.049263  0.000396\n",
      "PCA_7   0.049117  0.000415\n",
      "PCA_8   0.048678  0.000381\n",
      "PCA_9   0.048476  0.000392\n",
      "PCA_2   0.047525  0.000387\n",
      "PCA_12  0.047445  0.000393\n",
      "PCA_0   0.047337  0.000380\n"
     ]
    }
   ],
   "source": [
    "# 1. MDI\n",
    "clf_fit = clf.fit(X, y, sample_weight=weights)\n",
    "mdi_imp = feat_imp_mdi(clf_fit, col)\n",
    "print(mdi_imp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f20dcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MDA permuting features: 100%|██████████| 20/20 [37:45<00:00, 113.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean       std\n",
      "PCA_8   0.001491  0.000445\n",
      "PCA_17  0.001370  0.000586\n",
      "PCA_4   0.001166  0.000573\n",
      "PCA_19  0.000805  0.000300\n",
      "PCA_11  0.000736  0.000947\n",
      "PCA_10  0.000730  0.001512\n",
      "PCA_13  0.000630  0.000865\n",
      "PCA_7   0.000580  0.000947\n",
      "PCA_1   0.000416  0.001008\n",
      "PCA_3   0.000381  0.000432\n",
      "PCA_6   0.000376  0.000866\n",
      "PCA_18  0.000369  0.000743\n",
      "PCA_15  0.000005  0.001496\n",
      "PCA_16 -0.000230  0.000801\n",
      "PCA_5  -0.000251  0.000851\n",
      "PCA_9  -0.000368  0.000534\n",
      "PCA_12 -0.000480  0.000863\n",
      "PCA_14 -0.000639  0.000500\n",
      "PCA_2  -0.001096  0.001216\n",
      "PCA_0  -0.001827  0.001479 -0.7009380955929598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. MDA\n",
    "mda_imp, base = feat_imp_mda(\n",
    "    clf, X, y, cv=5,\n",
    "    sample_weight=weights,\n",
    "    t1=t1, pct_embargo=0.01,\n",
    "    scoring='neg_log_loss'\n",
    ")\n",
    "print(mda_imp, base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21c9594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean         std\n",
      "PCA_16 -0.836891  0.00624592\n",
      "PCA_0    -0.8486   0.0191458\n",
      "PCA_17 -0.851275   0.0107346\n",
      "PCA_18 -0.851617   0.0136697\n",
      "PCA_5  -0.853323  0.00954717\n",
      "PCA_9  -0.857127   0.0134292\n",
      "PCA_15 -0.861897   0.0043069\n",
      "PCA_10 -0.863975   0.0094395\n",
      "PCA_6  -0.865283   0.0111665\n",
      "PCA_7  -0.865411   0.0114415\n",
      "PCA_2  -0.866265  0.00833982\n",
      "PCA_19 -0.866476  0.00921716\n",
      "PCA_3  -0.872402   0.0115352\n",
      "PCA_4  -0.875083  0.00595834\n",
      "PCA_8  -0.875461  0.00624876\n",
      "PCA_12 -0.882521  0.00575093\n",
      "PCA_11  -0.88269   0.0120514\n",
      "PCA_14  -0.88443   0.0143149\n",
      "PCA_13 -0.897128  0.00950199\n",
      "PCA_1  -0.909431   0.0287834\n"
     ]
    }
   ],
   "source": [
    "# 3. SFI\n",
    "sfi_imp = SFI(X.columns, clf, X, y, scoring='neg_log_loss', sample_weight=weights , cv=5, t1 = t1, pct_embargo=0.01)\n",
    "print(sfi_imp)\n",
    "sfi_imp.to_csv('sfi_imp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853043a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
