{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5febb32f",
   "metadata": {},
   "source": [
    "### **讀取資料**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAUUSD 在第2群\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"QuantCommon\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from utils.processing import get_dollar_bars\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clusters = pd.read_csv(\"results/clusters.csv\", index_col=0)\n",
    "print(f'XAUUSD 在第{clusters.loc[\"XAUUSD_M1\", \"cluster\"]}群')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53011ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AUDUSD...\n",
      "Filtered Dollar Bars Count: 184442\n",
      "Processing EURGBP...\n",
      "Filtered Dollar Bars Count: 182676\n",
      "Processing EURUSD...\n",
      "Filtered Dollar Bars Count: 186054\n",
      "Processing GBPUSD...\n",
      "Filtered Dollar Bars Count: 183075\n",
      "Processing HK50...\n",
      "Filtered Dollar Bars Count: 10739\n",
      "Processing NZDUSD...\n",
      "Filtered Dollar Bars Count: 67499\n",
      "Processing UK100...\n",
      "Filtered Dollar Bars Count: 11067\n",
      "Processing US2000...\n",
      "Filtered Dollar Bars Count: 12016\n",
      "Processing USDCAD...\n",
      "Filtered Dollar Bars Count: 68479\n",
      "Processing XAGUSD...\n",
      "Filtered Dollar Bars Count: 55366\n",
      "Processing XAUUSD...\n",
      "Filtered Dollar Bars Count: 54861\n"
     ]
    }
   ],
   "source": [
    "group = clusters[clusters[\"cluster\"] == clusters.loc[\"XAUUSD_M1\", \"cluster\"]]\n",
    "data = dict({})\n",
    "for i in group.index:\n",
    "    print(f\"Processing {i[:-3]}...\")\n",
    "    filepath = os.path.join(project_root, \"data\", \"FI\", \"M1\",f\"{i}.csv\")\n",
    "    df = pd.read_csv(filepath, parse_dates=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = get_dollar_bars(df)\n",
    "    data[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9dfb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AUDUSD...\n",
      "Processing EURGBP...\n",
      "Processing EURUSD...\n",
      "Processing GBPUSD...\n",
      "Processing HK50...\n",
      "Processing NZDUSD...\n",
      "Processing UK100...\n",
      "Processing US2000...\n",
      "Processing USDCAD...\n",
      "Processing XAGUSD...\n",
      "Processing XAUUSD...\n"
     ]
    }
   ],
   "source": [
    "from utils.metalabeling import add_vertical_barrier, get_events, get_bins\n",
    "from utils.processing import apply_cusum_filter, getDailyVol, cal_weights, compute_talib_features\n",
    "\n",
    "feats_list, labels_list, weights_list, t1_list = [], [], [], []\n",
    "\n",
    "for symbol,df in data.items():\n",
    "    print(f\"Processing {symbol[:-3]}...\")\n",
    "    vol = getDailyVol(df[\"close\"], span0=20)\n",
    "    cusum_events  = apply_cusum_filter(df, volatility=vol).index\n",
    "    vertical_barriers = add_vertical_barrier(cusum_events, df, num_days=2)\n",
    "    pt_sl = [1, 1]\n",
    "    min_ret = 0.003\n",
    "    triple_barrier_events = get_events(close=df[\"close\"],\n",
    "                                                t_events=cusum_events,\n",
    "                                                pt_sl=pt_sl,\n",
    "                                                target=vol,\n",
    "                                                min_ret=min_ret,\n",
    "                                                num_threads=4,\n",
    "                                                vertical_barrier_times=vertical_barriers,\n",
    "                                                side_prediction=None)\n",
    "    labels  = get_bins(triple_barrier_events, df[\"close\"])\n",
    "    weights = cal_weights(triple_barrier_events, df[\"close\"])\n",
    "    feats = compute_talib_features(df,\n",
    "                               periods=[7,28,50,100],\n",
    "                               apply_ffd=True)\n",
    "    \n",
    "    # normalize features\n",
    "    for col in feats.columns:\n",
    "        # 每個 col 分別做 rolling.apply\n",
    "        feats[col] = (\n",
    "            feats[col]\n",
    "            .rolling(window=200, min_periods=1)\n",
    "            .apply(lambda arr: (arr <= arr[-1]).sum() / len(arr), raw=True)\n",
    "        )\n",
    "    idx = feats.index.intersection(labels.index)\n",
    "    feats = feats.loc[idx]\n",
    "    labels = labels.loc[idx][\"bin\"]\n",
    "    weights = weights.loc[idx][\"weight\"]\n",
    "    weights = weights / weights.mean() # normalize weights\n",
    "    t1 = triple_barrier_events.loc[idx][\"t1\"]\n",
    "\n",
    "    feats_list.append(feats)\n",
    "    labels_list.append(labels.rename(\"bin\"))\n",
    "    weights_list.append(weights.rename(\"weight\"))\n",
    "    t1_list.append(t1.rename(\"t1\"))\n",
    "\n",
    "    \n",
    "feats = pd.concat(feats_list)\n",
    "labels = pd.concat(labels_list)\n",
    "weights = pd.concat(weights_list)/len(weights)\n",
    "t1 = pd.concat(t1_list)\n",
    "\n",
    "combined_features = pd.concat(\n",
    "    [feats, labels, weights, t1],\n",
    "    axis=1\n",
    ")\n",
    "combined_features.sort_index(inplace=True)\n",
    "combined_features.to_csv(\"intermediate_results/combined_features.csv\", index=True)\n",
    "\n",
    "labels = combined_features['bin']   \n",
    "weights = combined_features['weight']\n",
    "t1 = combined_features['t1']\n",
    "feats = combined_features.drop(columns=['bin', 'weight', 't1'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b3aad",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83b73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === Pipeline : z-score → PCA ===\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\",    PCA(n_components=0.95, whiten=False)),\n",
    "])\n",
    "\n",
    "\n",
    "X = pipe.fit_transform(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d462cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pipeline_scaler_pca.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "# 1) 存整個 Pipeline（包含 scaler + pca）\n",
    "dump(pipe, \"pipeline_scaler_pca.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4fde4",
   "metadata": {},
   "source": [
    "## MDA MDI SFI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226a295",
   "metadata": {},
   "source": [
    "#### Purged K Fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d762fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PurgedKFold:\n",
    "    def __init__(self, n_splits=3, t1=None, pct_embargo=0.0):\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError(\"t1 must be a pandas Series\")\n",
    "        self.n_splits = n_splits\n",
    "        self.t1 = t1.sort_index()\n",
    "        self.pct_embargo = pct_embargo\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if not X.index.equals(self.t1.index):\n",
    "            raise ValueError(\"X and t1 must have the same index\")\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        # divide indices into contiguous chunks\n",
    "        test_slices = np.array_split(indices, self.n_splits)\n",
    "        mbrg = int(n_samples * self.pct_embargo)\n",
    "\n",
    "        for slice_ in test_slices:\n",
    "            i, j = slice_[0], slice_[-1] + 1\n",
    "            test_idx = indices[i:j]\n",
    "\n",
    "            # start‐time of test block\n",
    "            t0 = self.t1.index[i]\n",
    "            # end‐time of test block\n",
    "            t1_max = self.t1.iloc[test_idx].max()\n",
    "            # find the position just after t1_max\n",
    "            max_t1_pos = self.t1.index.searchsorted(t1_max)\n",
    "\n",
    "            # training before test block\n",
    "            train_before = indices[self.t1.index < t0]\n",
    "            # training after test + embargo\n",
    "            train_after = indices[max_t1_pos + mbrg :]\n",
    "\n",
    "            train_idx = np.concatenate([train_before, train_after])\n",
    "            yield train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4193bf1",
   "metadata": {},
   "source": [
    "#### CVscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b61fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "def cv_score(clf,\n",
    "             X,\n",
    "             y,\n",
    "             sample_weight=None,\n",
    "             scoring=\"neg_log_loss\",\n",
    "             t1=None,\n",
    "             cv=3,\n",
    "             pct_embargo=0.01):\n",
    "\n",
    "    if scoring not in [\"neg_log_loss\", \"accuracy\"]:\n",
    "        raise ValueError('scoring must be \"neg_log_loss\" or \"accuracy\"')\n",
    "\n",
    "    pkf = PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in pkf.split(X):\n",
    "        # 複製一份新的 model\n",
    "        model = clone(clf)\n",
    "        # fit\n",
    "        if sample_weight is None:\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        else:\n",
    "            model.fit(X.iloc[train_idx],\n",
    "                      y.iloc[train_idx],\n",
    "                      sample_weight=sample_weight.iloc[train_idx].values)\n",
    "        # predict + score\n",
    "        if scoring == \"neg_log_loss\":\n",
    "            prob = model.predict_proba(X.iloc[test_idx])\n",
    "            sc = -log_loss(y.iloc[test_idx],\n",
    "                           prob,\n",
    "                           sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values),\n",
    "                           labels=model.classes_)\n",
    "        else:\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            sc = accuracy_score(y.iloc[test_idx],\n",
    "                                pred,\n",
    "                                sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values))\n",
    "        scores.append(sc)\n",
    "    return np.array(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972226d",
   "metadata": {},
   "source": [
    "#### MDA MDI SFI 實作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a2b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# 1) MDI \n",
    "def feat_imp_mdi(fit, feat_names):\n",
    "    \"\"\"\n",
    "    fit: 已訓練好的 tree‐ensemble（RandomForest, ExtraTrees…）\n",
    "    feat_names: list of feature names\n",
    "    return: pd.DataFrame with columns [\"mean\",\"std\"] 純量化後的重要度\n",
    "    \"\"\"\n",
    "    # 從每顆樹蒐集 feature_importances_\n",
    "    df0 = pd.DataFrame(\n",
    "        [tree.feature_importances_ for tree in fit.estimators_],\n",
    "        columns=feat_names\n",
    "    ).replace(0, np.nan)  # 如果 max_features=1，某些 tree 有 0\n",
    "    imp = pd.concat({\n",
    "        \"mean\": df0.mean(),\n",
    "        \"std\" : df0.std() * df0.shape[0]**-0.5\n",
    "    }, axis=1)\n",
    "    # normalize to sum=1\n",
    "    imp[\"mean\"] /= imp[\"mean\"].sum()\n",
    "    imp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp\n",
    "\n",
    "\n",
    "# 2) MDA: \n",
    "def feat_imp_mda(clf,\n",
    "                 X,\n",
    "                 y,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = \"neg_log_loss\"\n",
    "                ) -> (pd.DataFrame, float):\n",
    "    # --- 1) numpy → pandas ---\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    feat_names = list(X.columns)\n",
    "\n",
    "    # --- 2) baseline score ---\n",
    "    base_scores = cv_score(clf, X, y,\n",
    "                           sample_weight=sample_weight,\n",
    "                           scoring=scoring,\n",
    "                           t1=t1,\n",
    "                           cv=cv,\n",
    "                           pct_embargo=pct_embargo)\n",
    "    base_mean = base_scores.mean()\n",
    "\n",
    "    # --- 3) 每個 feature permutation, 加進度條 ---\n",
    "    diffs = []\n",
    "    for col in tqdm(feat_names, desc=\"MDA permuting features\"):\n",
    "        Xp = X.copy()\n",
    "        np.random.shuffle(Xp[col].values)\n",
    "        perm_scores = cv_score(clf, Xp, y,\n",
    "                               sample_weight=sample_weight,\n",
    "                               scoring=scoring,\n",
    "                               t1=t1,\n",
    "                               cv=cv,\n",
    "                               pct_embargo=pct_embargo)\n",
    "        diffs.append(base_scores - perm_scores)\n",
    "\n",
    "    diffs = np.vstack(diffs)\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"mean\": diffs.mean(axis=1),\n",
    "        \"std\" : diffs.std(axis=1) * diffs.shape[1]**-0.5\n",
    "    }, index=feat_names)\n",
    "    imp_df.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp_df\n",
    "\n",
    "# 3) SFI\n",
    "def SFI(feat_names: list,\n",
    "                 clf,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = \"neg_log_loss\"\n",
    "                ) -> pd.DataFrame:\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X, columns=feat_names)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    imp = pd.DataFrame(columns=[\"mean\", \"std\"])\n",
    "    for featName in feat_names:\n",
    "        dfo = cv_score(clf, X=X[[featName]],  y = y,\n",
    "                      sample_weight= sample_weight,\n",
    "                      scoring=scoring, t1 = t1, cv = cv)\n",
    "        imp.loc[featName, \"mean\"] = dfo.mean()\n",
    "        imp.loc[featName, \"std\"] = dfo.std() * dfo.shape[0]**-0.5\n",
    "        imp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630b0e3",
   "metadata": {},
   "source": [
    "### RF and compute MDI MDA SFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c723464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [f\"PCA_{i}\" for i in range(X.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963a12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用CV不用切割資料集\n",
    "X = pd.DataFrame(X, columns= col, index=feats.index)\n",
    "y = labels.values\n",
    "weights = weights.values\n",
    "# t1 在上面定義好了　　　　　　　　　　　　　　　　　　　　　　　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af35c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf0c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgU = weights.mean()\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_features=\"sqrt\", class_weight=\"balanced\")\n",
    "clf = BaggingClassifier(estimator=clf, n_estimators=1000, max_samples=avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c73d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MDI\n",
    "clf_fit = clf.fit(X, y, sample_weight=weights)\n",
    "mdi_imp = feat_imp_mdi(clf_fit, col)\n",
    "mdi_imp.to_csv(\"results/mdi.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MDA permuting features: 100%|██████████| 32/32 [20:09<00:00, 37.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean       std\n",
      "PCA_26  0.000554  0.000265\n",
      "PCA_22  0.000510  0.000323\n",
      "PCA_29  0.000479  0.000327\n",
      "PCA_9   0.000414  0.000528\n",
      "PCA_30  0.000374  0.000244\n",
      "PCA_17  0.000368  0.000251\n",
      "PCA_21  0.000302  0.000115\n",
      "PCA_11  0.000281  0.000456\n",
      "PCA_19  0.000256  0.000239\n",
      "PCA_31  0.000254  0.000297\n",
      "PCA_23  0.000225  0.000240\n",
      "PCA_3   0.000213  0.000142\n",
      "PCA_16  0.000204  0.000309\n",
      "PCA_25  0.000198  0.000131\n",
      "PCA_14  0.000163  0.000148\n",
      "PCA_13  0.000155  0.000251\n",
      "PCA_8   0.000150  0.000401\n",
      "PCA_10  0.000132  0.000285\n",
      "PCA_6   0.000125  0.000372\n",
      "PCA_5   0.000118  0.000341\n",
      "PCA_28  0.000077  0.000224\n",
      "PCA_20  0.000049  0.000159\n",
      "PCA_2   0.000018  0.000151\n",
      "PCA_18  0.000015  0.000249\n",
      "PCA_1   0.000011  0.000236\n",
      "PCA_12  0.000002  0.000306\n",
      "PCA_24 -0.000015  0.000293\n",
      "PCA_0  -0.000030  0.000475\n",
      "PCA_4  -0.000043  0.000279\n",
      "PCA_27 -0.000109  0.000370\n",
      "PCA_15 -0.000117  0.000259\n",
      "PCA_7  -0.000269  0.000315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. MDA\n",
    "mda_imp = feat_imp_mda(\n",
    "    clf, X, y, cv=5,\n",
    "    sample_weight=weights,\n",
    "    t1=t1, pct_embargo=0.01,\n",
    "    scoring=\"neg_log_loss\"\n",
    ")\n",
    "print(mda_imp)\n",
    "mda_imp.to_csv(\"results/mda.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            mean       std\n",
      "PCA_10   -0.6933  0.000128\n",
      "PCA_29 -0.693366  0.000169\n",
      "PCA_28 -0.693376  0.000141\n",
      "PCA_14 -0.693436  0.000162\n",
      "PCA_8  -0.693591  0.000175\n",
      "PCA_31 -0.693601  0.000129\n",
      "PCA_30 -0.693672  0.000322\n",
      "PCA_18 -0.693696  0.000249\n",
      "PCA_21 -0.693717  0.000196\n",
      "PCA_5  -0.693745  0.000309\n",
      "PCA_22 -0.693759  0.000507\n",
      "PCA_19 -0.693772  0.000115\n",
      "PCA_27 -0.693784   0.00021\n",
      "PCA_7  -0.693794  0.000253\n",
      "PCA_9  -0.693801  0.000187\n",
      "PCA_2  -0.693804   0.00017\n",
      "PCA_0  -0.693808  0.000288\n",
      "PCA_20 -0.693824  0.000234\n",
      "PCA_26 -0.693834  0.000402\n",
      "PCA_6  -0.693836   0.00028\n",
      "PCA_4  -0.693848  0.000227\n",
      "PCA_1   -0.69386  0.000109\n",
      "PCA_16 -0.693867  0.000268\n",
      "PCA_24 -0.693968  0.000133\n",
      "PCA_17 -0.693972  0.000584\n",
      "PCA_25 -0.693977  0.000191\n",
      "PCA_15 -0.694074  0.000331\n",
      "PCA_3  -0.694074  0.000271\n",
      "PCA_23 -0.694118  0.000226\n",
      "PCA_11 -0.694134  0.000245\n",
      "PCA_13 -0.694147  0.000296\n",
      "PCA_12 -0.694167  0.000199\n"
     ]
    }
   ],
   "source": [
    "# 3. SFI\n",
    "sfi_imp = SFI(\n",
    "    X.columns, clf, X, y, \n",
    "    scoring=\"neg_log_loss\", \n",
    "    sample_weight=weights , \n",
    "    cv=5, t1 = t1, pct_embargo=0.01)\n",
    "print(sfi_imp)\n",
    "sfi_imp.to_csv(\"results/sfi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853043a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
