{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e307b435",
   "metadata": {},
   "source": [
    "## **Project2：特徵重要性計算 (Feature Importance)**\n",
    "\n",
    "### **動機**\n",
    "\n",
    "在量化交易策略的開發過程中，金融市場資料常包含大量技術指標與衍生特徵，若直接將所有特徵丟入機器學習模型，容易造成過度擬合、運算成本過高，且不易洞察哪些特徵對預測最具貢獻。  \n",
    "本專案旨在針對多樣化的技術指標與統計特徵，透過多種特徵重要性方法（MDI、MDA、SFI）進行評估，篩選出對後續 Meta-Labeling 或其他模型訓練最具代表性的特徵子集。\n",
    "\n",
    "---\n",
    "\n",
    "### **流程概述**\n",
    "\n",
    "1. **資料讀取與事件生成**  \n",
    "   - 讀取 Project1 產生的分群結果。  \n",
    "   - 將原始的多商品價格資料（Dollar Bars）與對應事件整合，確保「事件時間區間」與「標籤（Label）」可供後續特徵計算使用。\n",
    "\n",
    "2. **技術指標與特徵工程**  \n",
    "   - 對每段事件存續期間，以及其觸發前後指定的回溯窗口，計算多種技術指標（如 RSI、ATR、SMA、EMA、布林通道、成交量相關指標等）。  \n",
    "   - 對非平穩指標進行Fraction Difference，平穩化指標的同時讓指標保持記憶性。\n",
    "   - 使用PCA移除特徵間的共線性，以此降低特徵間的substition effect。\n",
    "\n",
    "3. **切分訓練與驗證資料**  \n",
    "   - 依據時間順序，將事件資料按「Purged K-Fold Cross-Validation」原則進行切分，以避免資料洩漏：  \n",
    "     1. 首先以時間區間（例如年份）做粗略切分；  \n",
    "     2. 在每個摺疊（Fold）內摒棄與測試集事件存續期間有交集的訓練事件（Purging）；  \n",
    "     3. 用唯一性(uniqueness)進一步平衡樣本權重。  \n",
    "   - 最終產生多組 Train / Validation 折疊，用於特徵重要性計算。\n",
    "\n",
    "4. **計算特徵重要性**  \n",
    "   - **MDI（Mean Decrease in Impurity）**：  \n",
    "     - 以隨機森林（Random Forest）模型擬合訓練集，根據每棵決策樹的節點分裂所減少的Entropy，累計至每個特徵。  \n",
    "   - **MDA（Mean Decrease in Accuracy，Permutation Importance）**：  \n",
    "     - 在驗證集上，逐一打亂每個特徵的取值（Permutation），觀察模型預測準確度的下降幅度。下降越多，代表該特徵對模型性能貢獻越大。  \n",
    "   - **SFI（Single Feature Importance）**：  \n",
    "     - 單獨僅使用某個特徵訓練模型，評估其在驗證集上的分類效果，排名特徵的預測能力。\n",
    "\n",
    "5. **結果可視化與輸出**  \n",
    "   - 繪製各特徵方法的排名條形圖（Bar Chart）。  \n",
    "   - 最終以 CSV 檔形式輸出`mdi.csv` `mda.csv` `sfi.csv`。。\n",
    "\n",
    "---\n",
    "\n",
    "### **核心特色說明**\n",
    "\n",
    "- **Purged K-Fold Cross-Validation**  \n",
    "  - 為避免事件期間與測試集重疊造成資料洩漏，每個摺疊會刪除掉與測試時段相重疊的訓練事件。  \n",
    "  - 根據《Advances in Financial Machine Learning》建議，先將事件按時間切分，再剔除重疊範圍，確保不會偷看到未來資料。\n",
    "\n",
    "- **Fractional Differentiation (FFD)**  \n",
    "  - 傳統的差分 (integer differencing) 可能過度去除序列長期記憶性，使得部分潛在訊號流失。FFD 提供一種「非整數階差分」的方式，透過逐步逼近法，根據最小化自相關與 ADF 單根檢定結果，選擇最佳差分階數。  \n",
    "  - FFD 的優點在於：  \n",
    "    1. **保留長期記憶性 (Long Memory)**：避免完全消除序列的自相關結構；  \n",
    "    2. **平穩化（Stationarize）**：將非平穩序列轉為近似平穩，符合大多回歸模型與機器學習對輸入特徵的假設；  \n",
    "    3. **減少信息損失**：相較於 Integer Differencing，可保留更多序列原始波動信息。  \n",
    "\n",
    "- **PCA 移除共線性**  \n",
    "  - 金融技術指標之間往往高度相關，若直接投入模型訓練，容易因替代現象 (Substitution Effect) 造成某些變量重要度被高估。  \n",
    "  - 使用 PCA（Principal Component Analysis）提取主要成分，保留 > 95% 變異量，同時降低維度，有助於後續特徵重要性計算更穩定。\n",
    "\n",
    "- **三種特徵重要性方法**  \n",
    "  1. **MDI（Mean Decrease in Impurity）**  \n",
    "     - 透過 Random Forest 訓練時，決策樹分裂節點所減少的 Gini 或 Entropy，作為特徵貢獻值的指標。  \n",
    "  2. **MDA（Permutation Importance）**  \n",
    "     - 在驗證集上逐一打亂特徵，觀察模型`neg_log_loss` 的下降量，下降越多則features更有貢獻。  \n",
    "  3. **SFI（Single Feature Importance）**  \n",
    "     - 單變數模型訓練，檢視該特徵單獨預測能力，提供最直觀的「單一特徵力量」評估。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febb32f",
   "metadata": {},
   "source": [
    "### **讀取資料**\n",
    "- 讀取黃金所在群集，並將該集合內資料轉換為dollar bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdaae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XAUUSD 在第2群\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(\"..\", \"QuantCommon\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# utils為我自己編寫的常用工具庫，檔案不在此作品集內\n",
    "from utils.processing import get_dollar_bars \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clusters = pd.read_csv(\"results/clusters.csv\", index_col=0)\n",
    "print(f'XAUUSD 在第{clusters.loc[\"XAUUSD_M1\", \"cluster\"]}群')\n",
    "\n",
    "group = clusters[clusters[\"cluster\"] == clusters.loc[\"XAUUSD_M1\", \"cluster\"]]\n",
    "data = dict({})\n",
    "for i in group.index:\n",
    "    print(f\"Processing {i[:-3]}...\")\n",
    "    filepath = os.path.join(project_root, \"data\", \"FI\", \"M1\",f\"{i}.csv\")\n",
    "    df = pd.read_csv(filepath, parse_dates=True)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df = get_dollar_bars(df)\n",
    "    data[i] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d3bc3",
   "metadata": {},
   "source": [
    "### **對資料進行labeling和特徵計算**\n",
    "- 使用trible barrier進行labeling以及計算指標作為特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f9dfb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AUDUSD...\n",
      "Processing EURGBP...\n",
      "Processing EURUSD...\n",
      "Processing GBPUSD...\n",
      "Processing HK50...\n",
      "Processing NZDUSD...\n",
      "Processing UK100...\n",
      "Processing US2000...\n",
      "Processing USDCAD...\n",
      "Processing XAGUSD...\n",
      "Processing XAUUSD...\n"
     ]
    }
   ],
   "source": [
    "from utils.metalabeling import add_vertical_barrier, get_events, get_bins\n",
    "from utils.processing import apply_cusum_filter, getDailyVol, cal_weights, compute_talib_features\n",
    "\n",
    "feats_list, labels_list, weights_list, t1_list = [], [], [], []\n",
    "\n",
    "for symbol,df in data.items():\n",
    "    print(f\"Processing {symbol[:-3]}...\")\n",
    "    vol = getDailyVol(df[\"close\"], span0=20)\n",
    "    cusum_events  = apply_cusum_filter(df, volatility=vol).index\n",
    "    vertical_barriers = add_vertical_barrier(cusum_events, df, num_days=2)\n",
    "    pt_sl = [1, 1]\n",
    "    min_ret = 0.003\n",
    "    triple_barrier_events = get_events(close=df[\"close\"],\n",
    "                                                t_events=cusum_events,\n",
    "                                                pt_sl=pt_sl,\n",
    "                                                target=vol,\n",
    "                                                min_ret=min_ret,\n",
    "                                                num_threads=4,\n",
    "                                                vertical_barrier_times=vertical_barriers,\n",
    "                                                side_prediction=None)\n",
    "    labels  = get_bins(triple_barrier_events, df[\"close\"])\n",
    "    weights = cal_weights(triple_barrier_events, df[\"close\"])\n",
    "    feats = compute_talib_features(df,\n",
    "                               periods=[7,28,50,100],\n",
    "                               apply_ffd=True)\n",
    "    \n",
    "    # normalize features\n",
    "    for col in feats.columns:\n",
    "        # 每個 col 分別做 rolling.apply\n",
    "        feats[col] = (\n",
    "            feats[col]\n",
    "            .rolling(window=200, min_periods=1)\n",
    "            .apply(lambda arr: (arr <= arr[-1]).sum() / len(arr), raw=True)\n",
    "        )\n",
    "    idx = feats.index.intersection(labels.index)\n",
    "    feats = feats.loc[idx]\n",
    "    labels = labels.loc[idx][\"bin\"]\n",
    "    weights = weights.loc[idx][\"weight\"]\n",
    "    weights = weights / weights.mean() # normalize weights\n",
    "    t1 = triple_barrier_events.loc[idx][\"t1\"]\n",
    "\n",
    "    feats_list.append(feats)\n",
    "    labels_list.append(labels.rename(\"bin\"))\n",
    "    weights_list.append(weights.rename(\"weight\"))\n",
    "    t1_list.append(t1.rename(\"t1\"))\n",
    "\n",
    "    \n",
    "feats = pd.concat(feats_list)\n",
    "labels = pd.concat(labels_list)\n",
    "weights = pd.concat(weights_list)/len(weights)\n",
    "t1 = pd.concat(t1_list)\n",
    "\n",
    "combined_features = pd.concat(\n",
    "    [feats, labels, weights, t1],\n",
    "    axis=1\n",
    ")\n",
    "combined_features.sort_index(inplace=True)\n",
    "combined_features.to_csv(\"intermediate_results/combined_features.csv\", index=True)\n",
    "\n",
    "labels = combined_features['bin']   \n",
    "weights = combined_features['weight']\n",
    "t1 = combined_features['t1']\n",
    "feats = combined_features.drop(columns=['bin', 'weight', 't1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26545f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b3aad",
   "metadata": {},
   "source": [
    "### **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83b73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === Pipeline : z-score → PCA ===\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\",    PCA(n_components=0.95, whiten=False)),\n",
    "])\n",
    "\n",
    "\n",
    "X = pipe.fit_transform(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d462cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/pipeline_scaler_pca.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "# 儲存模型\n",
    "dump(pipe, \"models/pipeline_scaler_pca.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226a295",
   "metadata": {},
   "source": [
    "### **建立所需function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427eb3e",
   "metadata": {},
   "source": [
    "- PurgedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d762fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PurgedKFold:\n",
    "    def __init__(self, n_splits=3, t1=None, pct_embargo=0.0):\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError(\"t1 must be a pandas Series\")\n",
    "        self.n_splits = n_splits\n",
    "        self.t1 = t1.sort_index()\n",
    "        self.pct_embargo = pct_embargo\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        if not X.index.equals(self.t1.index):\n",
    "            raise ValueError(\"X and t1 must have the same index\")\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        # divide indices into contiguous chunks\n",
    "        test_slices = np.array_split(indices, self.n_splits)\n",
    "        mbrg = int(n_samples * self.pct_embargo)\n",
    "\n",
    "        for slice_ in test_slices:\n",
    "            i, j = slice_[0], slice_[-1] + 1\n",
    "            test_idx = indices[i:j]\n",
    "\n",
    "            # start‐time of test block\n",
    "            t0 = self.t1.index[i]\n",
    "            # end‐time of test block\n",
    "            t1_max = self.t1.iloc[test_idx].max()\n",
    "            # find the position just after t1_max\n",
    "            max_t1_pos = self.t1.index.searchsorted(t1_max)\n",
    "\n",
    "            # training before test block\n",
    "            train_before = indices[self.t1.index < t0]\n",
    "            # training after test + embargo\n",
    "            train_after = indices[max_t1_pos + mbrg :]\n",
    "\n",
    "            train_idx = np.concatenate([train_before, train_after])\n",
    "            yield train_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4193bf1",
   "metadata": {},
   "source": [
    "- CVscore\n",
    "    - 因sklearn本身的cv score在傳送weights會不一致，需要自己建立function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "def cv_score(clf,\n",
    "             X,\n",
    "             y,\n",
    "             sample_weight=None,\n",
    "             scoring=\"neg_log_loss\",\n",
    "             t1=None,\n",
    "             cv=3,\n",
    "             pct_embargo=0.01):\n",
    "\n",
    "    if scoring not in [\"neg_log_loss\", \"accuracy\"]:\n",
    "        raise ValueError('scoring must be \"neg_log_loss\" or \"accuracy\"')\n",
    "\n",
    "    pkf = PurgedKFold(n_splits=cv, t1=t1, pct_embargo=pct_embargo)\n",
    "    scores = []\n",
    "\n",
    "    for train_idx, test_idx in pkf.split(X):\n",
    "        # 複製一份新的 model\n",
    "        model = clone(clf)\n",
    "        # fit\n",
    "        if sample_weight is None:\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        else:\n",
    "            model.fit(X.iloc[train_idx],\n",
    "                      y.iloc[train_idx],\n",
    "                      sample_weight=sample_weight.iloc[train_idx].values)\n",
    "        # predict + score\n",
    "        if scoring == \"neg_log_loss\":\n",
    "            prob = model.predict_proba(X.iloc[test_idx])\n",
    "            sc = -log_loss(y.iloc[test_idx],\n",
    "                           prob,\n",
    "                           sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values),\n",
    "                           labels=model.classes_)\n",
    "        else:\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            sc = accuracy_score(y.iloc[test_idx],\n",
    "                                pred,\n",
    "                                sample_weight=(None if sample_weight is None else sample_weight.iloc[test_idx].values))\n",
    "        scores.append(sc)\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5972226d",
   "metadata": {},
   "source": [
    "- MDA MDI SFI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 1) MDI \n",
    "def feat_imp_mdi(fit, feat_names):\n",
    "    # 從每顆樹蒐集 feature_importances_\n",
    "    df0 = pd.DataFrame(\n",
    "        [tree.feature_importances_ for tree in fit.estimators_],\n",
    "        columns=feat_names\n",
    "    ).replace(0, np.nan)  # 如果 max_features=1，某些 tree 有 0\n",
    "    imp = pd.concat({\n",
    "        \"mean\": df0.mean(),\n",
    "        \"std\" : df0.std() * df0.shape[0]**-0.5\n",
    "    }, axis=1)\n",
    "    # normalize to sum=1\n",
    "    imp[\"mean\"] /= imp[\"mean\"].sum()\n",
    "    imp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp\n",
    "\n",
    "\n",
    "# 2) MDA: \n",
    "def feat_imp_mda(clf,\n",
    "                 X,\n",
    "                 y,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = \"neg_log_loss\"\n",
    "                ) -> (pd.DataFrame, float):\n",
    "    # --- 1) numpy → pandas ---\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    feat_names = list(X.columns)\n",
    "\n",
    "    # --- 2) baseline score ---\n",
    "    base_scores = cv_score(clf, X, y,\n",
    "                           sample_weight=sample_weight,\n",
    "                           scoring=scoring,\n",
    "                           t1=t1,\n",
    "                           cv=cv,\n",
    "                           pct_embargo=pct_embargo)\n",
    "    base_mean = base_scores.mean()\n",
    "\n",
    "    # --- 3) 每個 feature permutation, 加進度條 ---\n",
    "    diffs = []\n",
    "    for col in tqdm(feat_names, desc=\"MDA permuting features\"):\n",
    "        Xp = X.copy()\n",
    "        np.random.shuffle(Xp[col].values)\n",
    "        perm_scores = cv_score(clf, Xp, y,\n",
    "                               sample_weight=sample_weight,\n",
    "                               scoring=scoring,\n",
    "                               t1=t1,\n",
    "                               cv=cv,\n",
    "                               pct_embargo=pct_embargo)\n",
    "        diffs.append(base_scores - perm_scores)\n",
    "\n",
    "    diffs = np.vstack(diffs)\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"mean\": diffs.mean(axis=1),\n",
    "        \"std\" : diffs.std(axis=1) * diffs.shape[1]**-0.5\n",
    "    }, index=feat_names)\n",
    "    imp_df.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp_df\n",
    "\n",
    "# 3) SFI\n",
    "def SFI(feat_names: list,\n",
    "                 clf,\n",
    "                 X: pd.DataFrame,\n",
    "                 y: pd.Series,\n",
    "                 sample_weight=None,\n",
    "                 t1=None,\n",
    "                 cv: int = 5,\n",
    "                 pct_embargo: float = 0.01,\n",
    "                 scoring: str = \"neg_log_loss\"\n",
    "                ) -> pd.DataFrame:\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = pd.DataFrame(X, columns=feat_names)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y, index=X.index)\n",
    "    if sample_weight is not None and not isinstance(sample_weight, pd.Series):\n",
    "        sample_weight = pd.Series(sample_weight, index=X.index)\n",
    "    if t1 is not None and not isinstance(t1, pd.Series):\n",
    "        t1 = pd.Series(t1, index=X.index)\n",
    "\n",
    "    imp = pd.DataFrame(columns=[\"mean\", \"std\"])\n",
    "    for featName in feat_names:\n",
    "        dfo = cv_score(clf, X=X[[featName]],  y = y,\n",
    "                      sample_weight= sample_weight,\n",
    "                      scoring=scoring, t1 = t1, cv = cv)\n",
    "        imp.loc[featName, \"mean\"] = dfo.mean()\n",
    "        imp.loc[featName, \"std\"] = dfo.std() * dfo.shape[0]**-0.5\n",
    "        imp.sort_values(by=\"mean\", ascending=False, inplace=True)\n",
    "    return imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630b0e3",
   "metadata": {},
   "source": [
    "### **建立模型和計算特徵重要性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [f\"PCA_{i}\" for i in range(X.shape[1])]\n",
    "X = pd.DataFrame(X, columns= col, index=feats.index)\n",
    "y = labels.values\n",
    "weights = weights.values\n",
    "# t1 在上面定義好了　　　　　　　　　　　　　　　　　　　　　　　"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af35c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "avgU = weights.mean()\n",
    "tree = DecisionTreeClassifier(criterion=\"entropy\", max_features=\"sqrt\", class_weight=\"balanced\")\n",
    "RF = BaggingClassifier(estimator=tree, n_estimators=1000, max_samples=avgU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c73d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Decrease in Impurity (MDI):\n",
      "            mean       std\n",
      "PCA_28  0.036056  0.009382\n",
      "PCA_17  0.035991  0.009206\n",
      "PCA_9   0.034368  0.008391\n",
      "PCA_13  0.033959  0.009764\n",
      "PCA_29  0.033876  0.008070\n",
      "PCA_4   0.033355  0.008488\n",
      "PCA_10  0.032779  0.008886\n",
      "PCA_15  0.032716  0.008518\n",
      "PCA_19  0.032278  0.008728\n",
      "PCA_26  0.032180  0.007766\n",
      "PCA_24  0.032159  0.008717\n",
      "PCA_23  0.032131  0.008056\n",
      "PCA_30  0.031801  0.008355\n",
      "PCA_20  0.031785  0.008110\n",
      "PCA_22  0.031701  0.007981\n",
      "PCA_21  0.031630  0.008596\n",
      "PCA_12  0.031346  0.008854\n",
      "PCA_18  0.031295  0.009183\n",
      "PCA_14  0.031191  0.009223\n",
      "PCA_6   0.031179  0.009147\n",
      "PCA_27  0.031099  0.007842\n",
      "PCA_16  0.030587  0.009740\n",
      "PCA_11  0.029985  0.008939\n",
      "PCA_2   0.029756  0.008952\n",
      "PCA_7   0.029366  0.009330\n",
      "PCA_0   0.028944  0.007892\n",
      "PCA_3   0.028658  0.008977\n",
      "PCA_8   0.028592  0.009641\n",
      "PCA_1   0.028241  0.009165\n",
      "PCA_25  0.027978  0.008794\n",
      "PCA_31  0.026762  0.008687\n",
      "PCA_5   0.026258  0.008511\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. MDI\n",
    "RF_fit = RF.fit(X, y, sample_weight=weights)\n",
    "mdi_imp = feat_imp_mdi(RF_fit, col)\n",
    "print(\"Mean Decrease in Impurity (MDI):\")\n",
    "print(mdi_imp)\n",
    "mdi_imp.to_csv(\"results/mdi.csv\")\n",
    "\n",
    "mdi_imp = pd.read_csv(\"results/mdi.csv\", index_col=0)\n",
    "mdi_sorted = mdi_imp.sort_values(by=\"mean\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(mdi_sorted.index, mdi_sorted[\"mean\"], color=\"steelblue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"MDI Importance\")\n",
    "plt.title(\"Mean Decrease in Impurity (MDI) Feature Importances\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20dcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MDA permuting features: 100%|██████████| 32/32 [22:50<00:00, 42.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Decrease Accuracy (MDA):\n",
      "            mean       std\n",
      "PCA_18  0.000565  0.000259\n",
      "PCA_31  0.000479  0.000418\n",
      "PCA_10  0.000292  0.000242\n",
      "PCA_14  0.000275  0.000510\n",
      "PCA_15  0.000268  0.000549\n",
      "PCA_25  0.000225  0.000216\n",
      "PCA_9   0.000188  0.000427\n",
      "PCA_1   0.000181  0.000448\n",
      "PCA_8   0.000137  0.000617\n",
      "PCA_21  0.000131  0.000271\n",
      "PCA_27  0.000124  0.000252\n",
      "PCA_22  0.000067  0.000359\n",
      "PCA_28  0.000032  0.000280\n",
      "PCA_2   0.000018  0.000471\n",
      "PCA_29 -0.000027  0.000217\n",
      "PCA_4  -0.000029  0.000239\n",
      "PCA_11 -0.000030  0.000272\n",
      "PCA_19 -0.000031  0.000215\n",
      "PCA_6  -0.000052  0.000260\n",
      "PCA_30 -0.000089  0.000168\n",
      "PCA_13 -0.000120  0.000317\n",
      "PCA_0  -0.000144  0.000479\n",
      "PCA_24 -0.000165  0.000228\n",
      "PCA_5  -0.000169  0.000291\n",
      "PCA_3  -0.000179  0.000312\n",
      "PCA_17 -0.000247  0.000354\n",
      "PCA_12 -0.000263  0.000116\n",
      "PCA_23 -0.000286  0.000434\n",
      "PCA_20 -0.000317  0.000305\n",
      "PCA_7  -0.000360  0.000171\n",
      "PCA_26 -0.000537  0.000521\n",
      "PCA_16 -0.000571  0.000220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. MDA\n",
    "mda_imp = feat_imp_mda(\n",
    "    RF, X, y, cv=5,\n",
    "    sample_weight=weights,\n",
    "    t1=t1, pct_embargo=0.01,\n",
    "    scoring=\"neg_log_loss\"\n",
    ")\n",
    "print(\"Mean Decrease Accuracy (MDA):\")\n",
    "print(mda_imp)\n",
    "mda_imp.to_csv(\"results/mda.csv\")\n",
    "\n",
    "\n",
    "\n",
    "mda_imp = pd.read_csv(\"results/mda.csv\", index_col=0)\n",
    "\n",
    "mda_sorted = mda_imp.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "# 繪製長條圖並加上標準差誤差線\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    mda_sorted.index,\n",
    "    mda_sorted[\"mean\"],\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Permutation Importance (Mean Decrease in Accuracy)\")\n",
    "plt.title(\"Mean Decrease in Accuracy (MDA) Feature Importances\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Factor Importance (SFI):\n",
      "            mean       std\n",
      "PCA_15 -0.693138  0.000212\n",
      "PCA_20 -0.693355  0.000139\n",
      "PCA_29 -0.693374  0.000139\n",
      "PCA_1  -0.693443  0.000193\n",
      "PCA_22  -0.69352  0.000294\n",
      "PCA_11 -0.693526  0.000148\n",
      "PCA_9  -0.693595  0.000118\n",
      "PCA_7  -0.693615  0.000272\n",
      "PCA_10 -0.693628  0.000362\n",
      "PCA_3  -0.693682   0.00013\n",
      "PCA_18 -0.693729  0.000156\n",
      "PCA_4  -0.693793  0.000112\n",
      "PCA_8  -0.693797  0.000397\n",
      "PCA_5  -0.693808  0.000164\n",
      "PCA_13 -0.693848  0.000301\n",
      "PCA_19 -0.693925  0.000161\n",
      "PCA_17 -0.693972  0.000053\n",
      "PCA_28 -0.694027   0.00017\n",
      "PCA_14 -0.694036  0.000436\n",
      "PCA_16 -0.694053  0.000229\n",
      "PCA_2  -0.694056  0.000142\n",
      "PCA_6  -0.694077  0.000342\n",
      "PCA_31 -0.694181  0.000211\n",
      "PCA_26 -0.694219   0.00024\n",
      "PCA_27 -0.694267  0.000175\n",
      "PCA_23 -0.694285  0.000205\n",
      "PCA_24 -0.694302  0.000448\n",
      "PCA_25 -0.694311  0.000181\n",
      "PCA_0  -0.694333  0.000389\n",
      "PCA_21 -0.694346  0.000235\n",
      "PCA_30 -0.694497  0.000284\n",
      "PCA_12  -0.69463  0.000428\n"
     ]
    }
   ],
   "source": [
    "# 3. SFI\n",
    "sfi_imp = SFI(\n",
    "    X.columns, RF, X, y, \n",
    "    scoring=\"neg_log_loss\", \n",
    "    sample_weight=weights , \n",
    "    cv=5, t1 = t1, pct_embargo=0.01)\n",
    "print(\"Single Factor Importance (SFI):\")\n",
    "print(sfi_imp)\n",
    "sfi_imp.to_csv(\"results/sfi.csv\")\n",
    "\n",
    "sfi_imp = pd.read_csv(\"results/sfi.csv\", index_col=0)\n",
    "\n",
    "\n",
    "sfi_sorted = sfi_imp.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(\n",
    "    sfi_sorted.index,\n",
    "    sfi_sorted[\"mean\"],\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Single Feature Importance (Mean ± Std)\")\n",
    "plt.title(\"Single Factor Importance (SFI) Feature Importances\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853043a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
